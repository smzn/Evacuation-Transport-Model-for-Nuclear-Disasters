import geopandas as gpd
import folium
import pandas as pd
import random
import requests
import osmnx as ox
import matplotlib.pyplot as plt
import networkx as nx
from itertools import combinations
from shapely.geometry import Point
import csv
import numpy as np
from shapely.geometry import LineString
from geopy.distance import geodesic
import time
import os
import xml.etree.ElementTree as ET
import scipy.ndimage
import matplotlib.colors as mcolors
import statistics

class CVRP_Geography:
    def __init__(self, file_path, layer_name="town"):
        """
        åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
        :param file_path: TopoJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :param layer_name: èª­ã¿è¾¼ã‚€ãƒ¬ã‚¤ãƒ¤ãƒ¼åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "town"ï¼‰
        """
        self.file_path = file_path
        self.layer_name = layer_name
        self.gdf = None
        self.center_lat = None
        self.center_lon = None

    def load_data(self):
        """ TopoJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ """
        try:
            self.gdf = gpd.read_file(self.file_path, layer=self.layer_name)

            # CRSï¼ˆåº§æ¨™ç³»ï¼‰ã®è¨­å®šï¼ˆWGS84ï¼‰
            if self.gdf.crs is None:
                self.gdf.set_crs(epsg=4326, inplace=True)

            # åœ°å›³ã®ä¸­å¿ƒåº§æ¨™ã‚’å–å¾—
            self.center_lat = self.gdf.geometry.centroid.y.mean()
            self.center_lon = self.gdf.geometry.centroid.x.mean()

            print(f"æˆåŠŸ: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚({self.layer_name})")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    def save_csv(self, output_csv_path):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        :param output_csv_path: ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            columns_to_keep = ["PREF_NAME", "CITY_NAME", "S_NAME", "AREA", "JINKO", "SETAI", "X_CODE", "Y_CODE", "SUPPORT_NEEDS"]
            if all(col in self.gdf.columns for col in columns_to_keep):
                self.gdf[columns_to_keep].to_csv(output_csv_path, index=False, encoding='utf-8-sig')
                print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒ {output_csv_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
            else:
                print("å¿…è¦ãªã‚«ãƒ©ãƒ ãŒãƒ‡ãƒ¼ã‚¿å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    def generate_map(self, output_html_path):
        """
        Foliumã‚’ä½¿ç”¨ã—ã¦åœ°å›³ã‚’ç”Ÿæˆã—ã€HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        :param output_html_path: ä¿å­˜ã™ã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if self.gdf is None:
            print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return

        try:
            m = folium.Map(location=[self.center_lat, self.center_lon], zoom_start=13, tiles="cartodbpositron")

            # å„åœ°åŒºã«ãƒãƒ¼ã‚«ãƒ¼ã‚’è¨­ç½®
            for _, row in self.gdf.iterrows():
                s_name = row["S_NAME"]      # ç”ºå
                area = row["AREA"]          # é¢ç©
                jinko = row["JINKO"]         # äººå£
                x_code = row["X_CODE"]       # çµŒåº¦
                y_code = row["Y_CODE"]       # ç·¯åº¦
                support_needs = row["SUPPORT_NEEDS"] # è¦æ”¯æ´è€…äººæ•°

                # ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã«è¡¨ç¤ºã™ã‚‹å†…å®¹
                popup_text = f"""
                <b>ç”ºå:</b> {s_name}<br>
                <b>é¢ç©:</b> {area:.2f} mÂ²<br>
                <b>äººå£:</b> {jinko} äºº <br>
                <b>è¦æ”¯æ´è€…:</b> {support_needs} äºº
                """

                # ãƒãƒ¼ã‚«ãƒ¼ã®è¿½åŠ 
                folium.Marker(
                    location=[y_code, x_code],
                    popup=folium.Popup(popup_text, max_width=300),
                    icon=folium.Icon(color="blue", icon="info-sign")
                ).add_to(m)

            # GeoJSONãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¿½åŠ 
            folium.GeoJson(
                self.gdf.to_json(),
                style_function=lambda x: {'color': 'red', 'weight': 2, 'fillOpacity': 0.3}
            ).add_to(m)

            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            m.save(output_html_path)
            print(f"åœ°å›³ãŒ {output_html_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: åœ°å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")
    
    def assign_support_needs(self, total_support_needs):
        """
        ç·è¦æ”¯æ´è€…äººæ•°ã‚’åœ°åŒºã®äººå£æ¯”ç‡ã«åŸºã¥ã„ã¦å‰²ã‚ŠæŒ¯ã‚‹
        :param total_support_needs: å…¨ä½“ã®è¦æ”¯æ´è€…ã®ç·æ•°
        """
        if "JINKO" not in self.gdf.columns:
            print("ã‚¨ãƒ©ãƒ¼: äººå£ï¼ˆJINKOï¼‰ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return

        # äººå£ã®åˆè¨ˆã‚’å–å¾—
        total_population = self.gdf["JINKO"].sum()

        # å„åœ°åŒºã«è¦æ”¯æ´è€…ã‚’äººå£æ¯”ã§å‰²ã‚Šå½“ã¦
        self.gdf["SUPPORT_NEEDS"] = (self.gdf["JINKO"] / total_population * total_support_needs).round().astype(int)

        print("è¦æ”¯æ´è€…äººæ•°ã®å‰²ã‚Šå½“ã¦ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    def load_shelters(self, csv_file, city_office_info=None):
        """
        é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦ä¿å­˜
        :param csv_file: é¿é›£æ‰€æƒ…å ±ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            self.shelters_df = pd.read_csv(csv_file, encoding='shift_jis')

            # "ä¸€æ™‚é¿é›£æ‰€" ä»¥å¤–ã®é¿é›£æ‰€ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            self.shelters_df = self.shelters_df[self.shelters_df['å‚™è€ƒ'] != 'ä¸€æ¬¡é¿é›£æ‰€']
            # å¸‚å½¹æ‰€ã®æƒ…å ±ãŒæä¾›ã•ã‚ŒãŸå ´åˆã«è¿½åŠ 
            if city_office_info:
                city_office_df = pd.DataFrame([city_office_info])
                self.shelters_df = pd.concat([self.shelters_df, city_office_df], ignore_index=True)

            print(f"é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚å¯¾è±¡é¿é›£æ‰€æ•°: {len(self.shelters_df)}")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    def plot_shelters(self, output_html_path):
        """
        ä¸€æ™‚é¿é›£æ‰€ä»¥å¤–ã®é¿é›£æ‰€ã‚’åœ°å›³ä¸Šã«è¡¨ç¤ºã—ã€åç§°ã¨æƒ³å®šåå®¹äººæ•°ã‚’è¡¨ç¤ºã™ã‚‹
        :param output_html_path: ä¿å­˜ã™ã‚‹HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if self.shelters_df is None or self.shelters_df.empty:
            print("ã‚¨ãƒ©ãƒ¼: é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return

        try:
            # æ¬ æå€¤ã‚’é©åˆ‡ãªå€¤ã«ç½®ãæ›ãˆï¼ˆåå®¹äººæ•°ãŒä¸æ˜ã®å ´åˆã¯0ã«ã™ã‚‹ï¼‰
            self.shelters_df["æƒ³å®šåå®¹äººæ•°"] = self.shelters_df["æƒ³å®šåå®¹äººæ•°"].fillna(0).astype(int)
            m = folium.Map(location=[self.center_lat, self.center_lon], zoom_start=13, tiles="cartodbpositron")

            for _, row in self.shelters_df.iterrows():
                name = row["åç§°"]
                capacity = row["æƒ³å®šåå®¹äººæ•°"]
                lat = row["ç·¯åº¦"]
                lon = row["çµŒåº¦"]
                category = row["å‚™è€ƒ"]

                # å¸‚å½¹æ‰€ã¯èµ¤è‰²ã‚¢ã‚¤ã‚³ãƒ³ã€ä»–ã®é¿é›£æ‰€ã¯ç·‘è‰²ã‚¢ã‚¤ã‚³ãƒ³
                if category == "å¸‚å½¹æ‰€":
                    icon_color = "red"
                    icon_type = "info-sign"
                else:
                    icon_color = "green"
                    icon_type = "home"

                popup_text = f"<b>é¿é›£æ‰€:</b> {name}<br><b>æƒ³å®šåå®¹äººæ•°:</b> {int(capacity)} äºº<br><b>å‚™è€ƒ:</b> {category}"

                folium.Marker(
                    location=[lat, lon],
                    popup=folium.Popup(popup_text, max_width=300),
                    icon=folium.Icon(color=icon_color, icon=icon_type)
                ).add_to(m)

            m.save(output_html_path)
            print(f"é¿é›£æ‰€ã®åœ°å›³ãŒ {output_html_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: é¿é›£æ‰€ã®åœ°å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    def get_gsi_elevation(self, lat, lon):
        """ å›½åœŸåœ°ç†é™¢APIã‚’åˆ©ç”¨ã—ã¦æ¨™é«˜ã‚’å–å¾—ã—ã€ç„¡åŠ¹ãªå€¤ã‚’å‡¦ç† """
        url = f"https://cyberjapandata2.gsi.go.jp/general/dem/scripts/getelevation.php?lon={lon}&lat={lat}&outtype=JSON"
        try:
            response = requests.get(url)
            response.raise_for_status()  # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            data = response.json()

            # å–å¾—ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            print(f"å–å¾—ãƒ‡ãƒ¼ã‚¿ï¼ˆ{lat}, {lon}ï¼‰: {data}")

            if "elevation" in data:
                elevation = data["elevation"]

                # æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ãŒ '-----' ãªã©ã®ç„¡åŠ¹ãªå€¤ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if elevation == "-----" or elevation is None:
                    print(f"è­¦å‘Š: æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ ({lat}, {lon})")
                    return None  # ã¾ãŸã¯ return 0 ã«å¤‰æ›´å¯

                return round(float(elevation), 2)

            else:
                print(f"è­¦å‘Š: æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ ({lat}, {lon})")
                return None

        except requests.exceptions.RequestException as e:
            print(f"ã‚¨ãƒ©ãƒ¼: æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ ({lat}, {lon}) - {e}")
            return None
        except ValueError as ve:
            print(f"ã‚¨ãƒ©ãƒ¼: æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ{lat}, {lon}ï¼‰- {ve}")
            return None

    def assign_random_support_needs(self, output_csv_path, map_output_html):
        """ å„åœ°åŒºã®è¦æ”¯æ´è€…ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å‰²ã‚Šå½“ã¦ã€ä½ç½®æƒ…å ±ã¨æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šã—ã¦ä¿å­˜ """
        if self.gdf is None:
            print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return

        assigned_data = []

        # 1. å¸‚å½¹æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆã«è¿½åŠ 
        id_counter = 0  # idã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’0ã‹ã‚‰é–‹å§‹
        for _, row in self.shelters_df.iterrows():
            if row['å‚™è€ƒ'] == 'å¸‚å½¹æ‰€':
                entry_type = 'city_hall'
                elevation = self.get_gsi_elevation(row['ç·¯åº¦'], row['çµŒåº¦'])
                assigned_data.append({
                    'id': id_counter,  # idã‚’è¨­å®š
                    'type': entry_type,
                    'x': row['çµŒåº¦'],
                    'y': row['ç·¯åº¦'],
                    'z': elevation,  # elevationã‚’zã«å¤‰æ›´
                    'demand': 0,
                    'priority': '-',
                    'name': row['åç§°'],
                    'capacity': row.get('æƒ³å®šåå®¹äººæ•°', 0),
                    'remarks': row.get('å‚™è€ƒ', '')
                })
                id_counter += 1  # idã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

        # 2. ä¸€èˆ¬ã®é¿é›£æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        for _, row in self.shelters_df.iterrows():
            if row['å‚™è€ƒ'] != 'å¸‚å½¹æ‰€':
                entry_type = 'shelter'
                elevation = self.get_gsi_elevation(row['ç·¯åº¦'], row['çµŒåº¦'])
                assigned_data.append({
                    'id': id_counter,  # idã‚’è¨­å®š
                    'type': entry_type,
                    'x': row['çµŒåº¦'],
                    'y': row['ç·¯åº¦'],
                    'z': elevation,  # elevationã‚’zã«å¤‰æ›´
                    'demand': 0,
                    'priority': '-',
                    'name': row['åç§°'],
                    'capacity': row.get('æƒ³å®šåå®¹äººæ•°', 0),
                    'remarks': row.get('å‚™è€ƒ', '')
                })
                id_counter += 1  # idã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

        # 3. è¦é…æ…®è€…ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆä¿®æ­£ç‰ˆï¼‰
        for _, row in self.gdf.iterrows():
            support_needs = row['SUPPORT_NEEDS']
            polygon = row['geometry']  # ã‚·ã‚§ãƒ¼ãƒ—æƒ…å ±
            for i in range(support_needs):
                while True:
                    # ãƒãƒªã‚´ãƒ³å†…ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆ
                    minx, miny, maxx, maxy = polygon.bounds
                    lon = random.uniform(minx, maxx)
                    lat = random.uniform(miny, maxy)
                    random_point = Point(lon, lat)

                    # ãƒã‚¤ãƒ³ãƒˆãŒãƒãƒªã‚´ãƒ³å†…ã«ã‚ã‚‹ã‹ã‚’ç¢ºèª
                    if not polygon.contains(random_point):
                        continue

                    # æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦æœ‰åŠ¹æ€§ã‚’ç¢ºèª
                    elevation = self.get_gsi_elevation(lat, lon)
                    if elevation is not None:  # æ¨™é«˜ãŒæœ‰åŠ¹ã§ã‚ã‚Œã°ãƒ«ãƒ¼ãƒ—çµ‚äº†
                        break

                # æœ‰åŠ¹ãªåº§æ¨™ã¨æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã§è¿½åŠ 
                assigned_data.append({
                    'id': id_counter,
                    'type': 'client',
                    'x': lon,
                    'y': lat,
                    'z': elevation,
                    'demand': random.choice([1, 2]),
                    'priority': random.randint(1, 5),
                    'name': f"{row['S_NAME']}_{i+1}",
                    'capacity': 0,
                    'remarks': ''
                })
                id_counter += 1  # idã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ

        # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›ã—ã¦CSVä¿å­˜
        df_assigned = pd.DataFrame(assigned_data)
        df_assigned.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f"è¦æ”¯æ´è€…ãƒ‡ãƒ¼ã‚¿ã¨é¿é›£æ‰€æƒ…å ±ãŒ {output_csv_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

        # åœ°å›³ã®ä½œæˆ
        m = folium.Map(location=[self.center_lat, self.center_lon], zoom_start=13, tiles="cartodbpositron")
        for entry in assigned_data:
            if entry['type'] == 'client':
                popup_text = f"<b>åå‰:</b> {entry['name']}<br><b>äººæ•°:</b> {entry['demand']} äºº<br><b>å„ªå…ˆåº¦:</b> {entry['priority']}<br><b>æ¨™é«˜:</b> {entry['z']} m"
                color, icon = "red", "user"
            else:
                popup_text = f"<b>é¿é›£æ‰€:</b> {entry['name']}<br><b>æƒ³å®šåå®¹äººæ•°:</b> {entry['capacity']} äºº<br><b>æ¨™é«˜:</b> {entry['z']} m<br><b>å‚™è€ƒ:</b> {entry['remarks']}"
                color, icon = ("blue", "info-sign") if entry['type'] == 'city_hall' else ("green", "home")

            folium.Marker(
                location=[entry['y'], entry['x']],
                popup=folium.Popup(popup_text, max_width=300),
                icon=folium.Icon(color=color, icon=icon)
            ).add_to(m)

        m.save(map_output_html)
        print(f"åœ°å›³ãŒ {map_output_html} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
            
            
    def plot_colored_roads(self, graphml_file, output_filepath):
        try:
            G = ox.load_graphml(filepath=graphml_file)
            print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

            road_colors = {
                "trunk": "red",
                "primary": "blue",
                "secondary": "green",
                "tertiary": "orange",
            }

            fig, ax = plt.subplots(figsize=(12, 12))

            edge_groups = {}  # color: list of (xs, ys)
            for u, v, k, data in G.edges(keys=True, data=True):
                highway_type = data.get("highway")
                if isinstance(highway_type, list):
                    highway_type = highway_type[0]
                highway_type = highway_type or "other"
                color = road_colors.get(highway_type, "gray")

                if "geometry" in data:
                    xs, ys = data["geometry"].xy
                else:
                    # fallback to straight line if no geometry
                    x1, y1 = G.nodes[u]["x"], G.nodes[u]["y"]
                    x2, y2 = G.nodes[v]["x"], G.nodes[v]["y"]
                    xs, ys = [x1, x2], [y1, y2]

                edge_groups.setdefault(color, []).append((xs, ys))

            # æç”»
            for color, lines in edge_groups.items():
                for xs, ys in lines:
                    ax.plot(xs, ys, color=color, linewidth=2)

            # å‡¡ä¾‹
            legend_labels = {
                "trunk": "Trunk",
                "primary": "Primary",
                "secondary": "Secondary",
                "tertiary": "Tertiary",
                "gray": "Other"
            }
            used_colors = set(edge_groups.keys())
            handles = [plt.Line2D([0], [0], color=c, lw=2, label=legend_labels.get(k, "Other"))
                    for k, c in road_colors.items() if c in used_colors]
            if "gray" in used_colors:
                handles.append(plt.Line2D([0], [0], color="gray", lw=2, label="Other"))

            ax.legend(handles=handles, title="Road Types", loc="upper left")
            plt.title("Color-coded Road Types in Omaezaki City", fontsize=16)
            plt.axis("off")
            plt.savefig(output_filepath, dpi=300, bbox_inches="tight")
            print(f"åœ°å›³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_filepath}")

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: é“è·¯åœ°å›³ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}")

    def calculate_travel_times(self, graphml_file, nodes_csv, output_csv, output_matrix_csv, penalty=100000):
        """
        å…¨ã¦ã®ãƒãƒ¼ãƒ‰é–“ã®ç§»å‹•æ™‚é–“ã‚’è¨ˆç®—ã—ã€çµæœã‚’CSVã¨è¡Œåˆ—å½¢å¼ã§ä¿å­˜
        :param graphml_file: GraphMLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :param nodes_csv: ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :param output_csv: çµæœã‚’ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        :param output_matrix_csv: è¡Œåˆ—å½¢å¼ã®çµæœã‚’ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            # ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿
            print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            G = ox.load_graphml(filepath=graphml_file)
            print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

            # ã‚¨ãƒƒã‚¸ã«ç§»å‹•æ™‚é–“ï¼ˆweightï¼‰ã‚’è¿½åŠ 
            for u, v, k, data in G.edges(data=True, keys=True):
                # ã‚¨ãƒƒã‚¸ã®é•·ã•ã¨åˆ¶é™é€Ÿåº¦ã‚’å–å¾—
                length = data.get("length", 1)  # è·é›¢ (m)
                speed = data.get("maxspeed", 30)  # åˆ¶é™é€Ÿåº¦ (km/h)

                # maxspeedãŒãƒªã‚¹ãƒˆã®å ´åˆã€æœ€åˆã®å€¤ã‚’ä½¿ç”¨
                if isinstance(speed, list):
                    speed = speed[0]

                # åˆ¶é™é€Ÿåº¦ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                try:
                    speed = float(speed)
                except (TypeError, ValueError):
                    speed = 30  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ¶é™é€Ÿåº¦ (km/h)

                # ç§»å‹•æ™‚é–“ï¼ˆç§’ï¼‰ã‚’è¨ˆç®—ã—ã¦ã‚¨ãƒƒã‚¸ã«è¿½åŠ 
                travel_time = length / (speed * 1000 / 3600)  # ç§’å˜ä½ã®æ™‚é–“
                data["weight"] = travel_time

            # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            print("ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            nodes_df = pd.read_csv(nodes_csv)
            print("ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

            # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
            travel_times = []

            # å…¨ã¦ã®ãƒãƒ¼ãƒ‰é–“ã®çµ„ã¿åˆã‚ã›ã‚’å–å¾—
            node_pairs = combinations(nodes_df["id"], 2)

            # ãƒãƒ¼ãƒ‰IDã¨åº§æ¨™ã€åå‰ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
            id_to_coords = nodes_df.set_index("id")[["x", "y", "name"]].to_dict(orient="index")

            # å…¨ã¦ã®çµ„ã¿åˆã‚ã›ã§ç§»å‹•æ™‚é–“ã‚’è¨ˆç®—
            for source_id, target_id in node_pairs:
                try:
                    source_coords = id_to_coords[source_id]
                    target_coords = id_to_coords[target_id]

                    # èµ·ç‚¹ã¨çµ‚ç‚¹ã®ãƒãƒ¼ãƒ‰ã‚’å–å¾—
                    source_node = ox.distance.nearest_nodes(G, X=source_coords["x"], Y=source_coords["y"])
                    target_node = ox.distance.nearest_nodes(G, X=target_coords["x"], Y=target_coords["y"])

                    # æœ€çŸ­çµŒè·¯ã‚’è¨ˆç®—
                    route = nx.shortest_path(G, source_node, target_node, weight="weight")
                    travel_time = nx.shortest_path_length(G, source_node, target_node, weight="weight")

                    # çµæœã‚’ãƒªã‚¹ãƒˆã«ä¿å­˜
                    travel_times.append({
                        "source_id": source_id,
                        "target_id": target_id,
                        "travel_time": travel_time,
                    })

                    # è¨ˆç®—çŠ¶æ³ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«å‡ºåŠ›
                    print(f"è¨ˆç®—ä¸­: æ‹ ç‚¹ {source_id} -> æ‹ ç‚¹ {target_id} | ç§»å‹•æ™‚é–“: {travel_time:.2f} ç§’")

                except nx.NetworkXNoPath:
                    print(f"ãƒ«ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {source_id} -> {target_id}")
                    travel_times.append({
                        "source_id": source_id,
                        "target_id": target_id,
                        "travel_time": penalty,#ãƒ«ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç§»å‹•æ™‚é–“ã‚’å¤§ããã™ã‚‹
                    })

            # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
            travel_times_df = pd.DataFrame(travel_times)
            all_nodes = list(range(len(nodes_df)))
            # è¡Œåˆ—å½¢å¼ã«å¤‰æ›ã—ã¦ä¿å­˜
            travel_times_df = travel_times_df.set_index(["source_id", "target_id"]).reindex(
                pd.MultiIndex.from_product([all_nodes, all_nodes], names=["source_id", "target_id"]),
                fill_value=0
            ).reset_index()

            # çµæœã‚’CSVã«ä¿å­˜
            travel_times_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
            print(f"ç§»å‹•æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’ {output_csv} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

            travel_time_matrix = travel_times_df.pivot(index="source_id", columns="target_id", values="travel_time").fillna(0)
            print(f"Travel time matrix created with shape: {travel_time_matrix.shape}")
            travel_time_matrix.to_csv(output_matrix_csv, index=True, encoding="utf-8-sig")
            print(f"è¡Œåˆ—å½¢å¼ã®ç§»å‹•æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’ {output_matrix_csv} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            
            
    def calculate_travel_times2(self, graphml_file, nodes_csv, output_csv, output_matrix_csv, penalty=100000):
        """
        å…¨ã¦ã®ãƒãƒ¼ãƒ‰é–“ã®ç§»å‹•æ™‚é–“ã‚’è¨ˆç®—ã—ã€çµæœã‚’CSVã¨è¡Œåˆ—å½¢å¼ã§ä¿å­˜ + ãƒ«ãƒ¼ãƒˆã‚’è¿”ã™
        """
        try:
            G = ox.load_graphml(filepath=graphml_file)
            print("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

            for u, v, k, data in G.edges(data=True, keys=True):
                length = data.get("length", 1)
                speed = data.get("maxspeed", 30)
                if isinstance(speed, list):
                    speed = speed[0]
                try:
                    speed = float(speed)
                except (TypeError, ValueError):
                    speed = 30
                travel_time = length / (speed * 1000 / 3600)
                data["weight"] = travel_time

            nodes_df = pd.read_csv(nodes_csv)
            travel_times = []
            routes = []
            node_pairs = combinations(nodes_df["id"], 2)
            id_to_coords = nodes_df.set_index("id")[["x", "y", "name"]].to_dict(orient="index")

            for source_id, target_id in node_pairs:
                try:
                    source_coords = id_to_coords[source_id]
                    target_coords = id_to_coords[target_id]
                    source_node = ox.distance.nearest_nodes(G, X=source_coords["x"], Y=source_coords["y"])
                    target_node = ox.distance.nearest_nodes(G, X=target_coords["x"], Y=target_coords["y"])

                    route = nx.shortest_path(G, source_node, target_node, weight="weight")
                    travel_time = nx.shortest_path_length(G, source_node, target_node, weight="weight")

                    travel_times.append({
                        "source_id": source_id,
                        "target_id": target_id,
                        "travel_time": travel_time,
                    })
                    routes.append({
                        "source_id": source_id,
                        "target_id": target_id,
                        "route": route
                    })
                    print(f"è¨ˆç®—ä¸­: {source_id} -> {target_id} | {travel_time:.2f} ç§’")
                except nx.NetworkXNoPath:
                    print(f"ãƒ«ãƒ¼ãƒˆãªã—: {source_id} -> {target_id}")
                    travel_times.append({
                        "source_id": source_id,
                        "target_id": target_id,
                        "travel_time": penalty
                    })

            travel_times_df = pd.DataFrame(travel_times)
            all_nodes = list(range(len(nodes_df)))
            travel_times_df = travel_times_df.set_index(["source_id", "target_id"]).reindex(
                pd.MultiIndex.from_product([all_nodes, all_nodes], names=["source_id", "target_id"]),
                fill_value=0
            ).reset_index()

            travel_times_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
            print(f"ç§»å‹•æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’ {output_csv} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

            travel_time_matrix = travel_times_df.pivot(index="source_id", columns="target_id", values="travel_time").fillna(0)
            travel_time_matrix.to_csv(output_matrix_csv, index=True, encoding="utf-8-sig")
            print(f"è¡Œåˆ—å½¢å¼ã®ç§»å‹•æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‚’ {output_matrix_csv} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

            return routes  # â† ãƒ«ãƒ¼ãƒˆæƒ…å ±ã‚’è¿”ã™
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return []


    def set_vehicle_info(self, num_vehicles, vehicle_capacity, vehicle_file="omaezaki_vehicles.csv"):
        """
        è»Šä¸¡æƒ…å ±ã‚’è¨­å®šã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
        :param num_vehicles: è»Šä¸¡ã®å°æ•°
        :param vehicle_capacity: å„è»Šä¸¡ã®å®¹é‡
        :param vehicle_file: è»Šä¸¡æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«å
        """

        # è»Šä¸¡ã®ç”Ÿæˆ
        vehicles = [{"id": i, "capacity": vehicle_capacity} for i in range(num_vehicles)]
        df_vehicles = pd.DataFrame(vehicles)

        # CSVä¿å­˜
        df_vehicles.to_csv(vehicle_file, index=False, columns = ["id", "capacity"])

        return vehicles
    
    
    def interpolate_points(self, lat1, lon1, lat2, lon2, distance, interval_m=5):
        """ç·¯åº¦çµŒåº¦ã‚’æŒ‡å®šã—ã¦5mé–“éš”ã§è£œé–“ç‚¹ã‚’ä½œã‚‹"""
        start = np.array([lat1, lon1])
        end = np.array([lat2, lon2])

        # ç·è·é›¢ï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰
        num_points = int(distance // interval_m)

        if num_points <= 1:
            return [(lat1, lon1), (lat2, lon2)]

        lats = np.linspace(lat1, lat2, num_points + 1)
        lons = np.linspace(lon1, lon2, num_points + 1)
        return list(zip(lats, lons))
    
    def latlon_to_int_id(self, lat, lon):
        return int(lat * 1e7) * 10**9 + int(lon * 1e7)

    def get_filtered_road_network(self, include_types=None, exclude_types=None, output_file="filtered_network.graphml", elev=50, n=10, nrate=0.5):
        if include_types:
            custom_filter = '["highway"~"' + "|".join(include_types) + '"]'
        elif exclude_types:
            custom_filter = '["highway"!~"' + "|".join(exclude_types) + '"]'
        else:
            custom_filter = None

        G = ox.graph_from_place("Omaezaki, Shizuoka, Japan", network_type="drive", custom_filter=custom_filter)
        
        edges = list(G.edges(keys=True, data=True))
        total = len(edges)
        
        edges_to_remove = []
        distances = []
        for idx, (u, v, key, data) in enumerate(edges):
            if "highway" not in data:
                print(f"Edge {idx+1}: highwayãªã— â†’ å‰Šé™¤äºˆå®š")
                edges_to_remove.append((u, v, key))
        
        
        for idx, (u, v, key, data) in enumerate(edges):
            try:
                lat1, lon1 = G.nodes[u]['y'], G.nodes[u]['x']
                lat2, lon2 = G.nodes[v]['y'], G.nodes[v]['x']
                distance_m = geodesic((lat1, lon1), (lat2, lon2)).meters
                distances.append(distance_m)
                #print(f"Edge {idx+1}/{total}: from {u} to {v} = {distance_m:.2f} m")
                
                
                ####å¯¾è±¡edgeã‚’nåˆ†å‰²ã—ã¦rate%ãŒå¯¾è±¡æ¨™é«˜ä»¥ä¸‹ãªã‚‰ã°å‰Šé™¤ã™ã‚‹
                points = self.interpolate_points(lat1, lon1, lat2, lon2, distance_m, interval_m=round(distance_m/n,1))
                # å„ç‚¹ã«æ¨™é«˜ã‚’ä»˜ä¸
                points_with_elev = [(lat, lon, self.get_elevation_from_latlon(lat, lon)) for lat, lon in points]                
                # æ¡ä»¶ã«åˆã†ç‚¹ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                num_high = sum(1 for _, _, e in points_with_elev if e >= elev)
                # å…¨ä½“ã«å¯¾ã™ã‚‹å‰²åˆ
                ratio = num_high / len(points_with_elev)
                if ratio < nrate:
                    edges_to_remove.append((u, v, key))
                
                
                #egdeã‚’åŒºåˆ‡ã‚‹å ´åˆã¯ã†ã¾ãã„ã‹ãªã„
                """
                if distance_m > 100:
                    points = self.interpolate_points(lat1, lon1, lat2, lon2, distance_m, interval_m=50)
                    edges_to_remove.append((u, v, key))
                    # ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆè£œé–“ç‚¹ã«æ–°IDã‚’ä»˜ã‘ã‚‹ï¼‰
                    new_ids = []
                    for lat, lon in points:
                        nid = self.latlon_to_int_id(lat, lon)
                        if nid not in G.nodes:
                            G.add_node(nid, y=lat, x=lon)
                        new_ids.append(nid)
                    
                    # new_ids ã®å…ˆé ­ã¨æœ«å°¾ã‚’ u, v ã«æ¥ç¶šã™ã‚‹ï¼ˆæ¨™é«˜æ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã‚Œã°ï¼‰
                    start_pt = (G.nodes[new_ids[0]]['y'], G.nodes[new_ids[0]]['x'])
                    end_pt = (G.nodes[new_ids[-1]]['y'], G.nodes[new_ids[-1]]['x'])

                    start_elev = self.get_elevation_from_latlon(*start_pt)
                    u_elev = self.get_elevation_from_latlon(G.nodes[u]['y'], G.nodes[u]['x'])
                    if start_elev > elev and u_elev > elev:
                        G.add_edge(u, new_ids[0], **data)

                    end_elev = self.get_elevation_from_latlon(*end_pt)
                    v_elev = self.get_elevation_from_latlon(G.nodes[v]['y'], G.nodes[v]['x'])
                    if end_elev > elev and v_elev > elev:
                        G.add_edge(new_ids[-1], v, **data)
                        
                    # ãƒãƒ¼ãƒ‰é–“ã‚’æ–°ã—ã„ã‚¨ãƒƒã‚¸ã§ã¤ãªãï¼ˆæ¨™é«˜æ¡ä»¶ã‚ã‚Šï¼‰
                    for i in range(len(new_ids) - 1):
                        pt1 = (G.nodes[new_ids[i]]['y'], G.nodes[new_ids[i]]['x'])
                        pt2 = (G.nodes[new_ids[i+1]]['y'], G.nodes[new_ids[i+1]]['x'])
                        elev1 = self.get_elevation_from_latlon(*pt1)
                        elev2 = self.get_elevation_from_latlon(*pt2)
                        if elev1 > elev and elev2 > elev:
                            G.add_edge(new_ids[i], new_ids[i+1], **data)
                            # é€šå¸¸ã®ã‚¨ãƒƒã‚¸è¿½åŠ ã«åŠ ãˆã¦é€†æ–¹å‘ã‚‚è¿½åŠ 
                            G.add_edge(new_ids[i+1], new_ids[i], **data)
                
                else:
                    elev1 = self.get_elevation_from_latlon(lat1, lon1)
                    elev2 = self.get_elevation_from_latlon(lat2, lon2)
                    if not (elev1 > elev and elev2 > elev):
                        edges_to_remove.append((u, v, key))          
                """
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ on edge {u}-{v}: {e}")
                continue

            if idx % max(1, total // 20) == 0:
                percent = (idx + 1) / total * 100
                print(f"{percent:.1f}% å®Œäº†ï¼ˆ{idx + 1}/{total}ï¼‰")
                
        
        # ä¸è¦ãªã‚¨ãƒƒã‚¸ã‚’å‰Šé™¤
        G.remove_edges_from(edges_to_remove)
        print(f"{len(edges_to_remove)}/{len(edges)}ä»¶({len(edges_to_remove)/len(edges):.2})ã®ã‚¨ãƒƒã‚¸ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        
        
        ox.save_graphml(G, filepath=output_file)
        print(f"æ¨™é«˜{elev}mè¶…ã®éƒ¨åˆ†ã®ã¿ã§æ§‹æˆã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        print("egdeã”ã¨ã®è·é›¢")
        print("æœ€å¤§å€¤:", max(distances))
        print("æœ€å°å€¤:", min(distances))
        print("å¹³å‡:", statistics.mean(distances))
        print("ä¸­å¤®å€¤:", statistics.median(distances))
        print("æ¨™æº–åå·®:", statistics.stdev(distances))  # ä¸ååˆ†æ•£ã®æ¨™æº–åå·®
        return G
    
    def load_elev(self, npfile, csvfile):
        # æ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        self.elevation_array = np.load(npfile)

        # ç·¯åº¦çµŒåº¦ã‚’CSVã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆä¸Šè¨˜ã¨åŒæ§˜ï¼‰
        with open(csvfile, mode="r") as f:
            reader = csv.DictReader(f)
            row = next(reader)
            self.lat_min = float(row["lat_min"])
            self.lat_max = float(row["lat_max"])
            self.lon_min = float(row["lon_min"])
            self.lon_max = float(row["lon_max"])

        #print("âœ… elevation_array shape:", elevation_array.shape)
        #print("ğŸ“ ç·¯åº¦çµŒåº¦ç¯„å›²:", lat_min, "ï½", lat_max, " / ", lon_min, "ï½", lon_max)
        
    def get_elevation_from_latlon(self, lat, lon):
        """
        æŒ‡å®šã•ã‚ŒãŸç·¯åº¦çµŒåº¦ã‹ã‚‰æ¨™é«˜ã‚’å–å¾—ã™ã‚‹é–¢æ•°
        Get elevation from specified latitude and longitude.

        Parameters:
            lat (float): ç·¯åº¦ / Latitude
            lon (float): çµŒåº¦ / Longitude

        Returns:
            float: æŒ‡å®šåœ°ç‚¹ã®æ¨™é«˜ï¼ˆç¯„å›²å¤–ã¯ Noneï¼‰ / Elevation value or None if out of bounds
        """
        nrows, ncols = self.elevation_array.shape

        # ç¯„å›²ãƒã‚§ãƒƒã‚¯ / Check if coordinates are within bounds
        if not (self.lat_min <= lat <= self.lat_max) or not (self.lon_min <= lon <= self.lon_max):
            print("âŒ æŒ‡å®šã—ãŸåœ°ç‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å¤–ã§ã™ã€‚ / Location is outside data bounds.")
            return None

        # ç·¯åº¦ãƒ»çµŒåº¦ã‚’é…åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ› / Convert lat/lon to array indices
        row_ratio = (self.lat_max - lat) / (self.lat_max - self.lat_min)
        col_ratio = (lon - self.lon_min) / (self.lon_max - self.lon_min)

        row = int(round(row_ratio * (nrows - 1)))
        col = int(round(col_ratio * (ncols - 1)))

        # å®‰å…¨å¯¾ç­–ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒç¯„å›²å¤–ã«ãªã‚‰ãªã„ã‚ˆã†ã«åˆ¶é™ / Clip indices to valid range
        row = min(max(row, 0), nrows - 1)
        col = min(max(col, 0), ncols - 1)

        return self.elevation_array[row, col]

    # GMLæ¨™é«˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿é–¢æ•°ï¼ˆè£œé–“ã¯è¡Œã‚ãªã„ï¼‰
    # Function to parse GML elevation data (no interpolation here)
    def parse_gml_dem_10m(self, xml_file):
        ns = {'gml': "http://www.opengis.net/gml/3.2"}
        tree = ET.parse(xml_file)
        root = tree.getroot()

        try:
            # ç·¯åº¦çµŒåº¦ã®ç¯„å›²ã‚’å–å¾— / Get coordinate bounds
            lower_corner = root.find('.//gml:Envelope/gml:lowerCorner', ns).text.split()
            upper_corner = root.find('.//gml:Envelope/gml:upperCorner', ns).text.split()
            lat_min, lon_min = map(float, lower_corner)
            lat_max, lon_max = map(float, upper_corner)

            # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºå–å¾— / Get grid size
            grid_size = root.find('.//gml:GridEnvelope/gml:high', ns).text.split()
            grid_x, grid_y = map(int, grid_size)
            expected_size = (grid_x + 1) * (grid_y + 1)

            # æ¨™é«˜ãƒ‡ãƒ¼ã‚¿å–å¾— / Get elevation data
            elevation_values = root.find('.//gml:tupleList', ns).text.strip().split("\n")
            elevations = np.array([float(e.split(",")[1]) for e in elevation_values])
            elevations[elevations == -9999.0] = np.nan  # ç•°å¸¸å€¤ã‚’NaNã« / Replace -9999 with NaN

            # ã‚µã‚¤ã‚ºèª¿æ•´ / Adjust size if mismatched
            if elevations.size < expected_size:
                elevations = np.pad(elevations, (0, expected_size - elevations.size), mode='edge')
            elif elevations.size > expected_size:
                elevations = elevations[:expected_size]

            # é…åˆ—ã®å½¢çŠ¶ã‚’è¨­å®šï¼ˆç·¯åº¦ Ã— çµŒåº¦ï¼‰/ Reshape as (lat, lon)
            elevations = elevations.reshape((grid_y + 1, grid_x + 1))

            return {
                "filename": os.path.basename(xml_file),
                "elevations": elevations,
                "lat_range": (lat_min, lat_max),
                "lon_range": (lon_min, lon_max)
            }

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {xml_file} - {e}")
            return None


    # GMLã‚¿ã‚¤ãƒ«ã‚’çµåˆã—ã¦è£œé–“å‡¦ç†ã‚’ä¸€æ‹¬ã§è¡Œã†é–¢æ•°
    # Merge multiple GML tiles and apply interpolation afterward
    def merge_gml_elevation_tiles_10m(self, input_dir, output_prefix="merged_10m_elevation"):
        results = []
        for filename in os.listdir(input_dir):
            if filename.endswith('.xml'):
                filepath = os.path.join(input_dir, filename)
                result = self.parse_gml_dem_10m(filepath)
                if result:
                    results.append(result)

        if not results:
            print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ / No valid data found.")
            return

        print(f"ğŸ“¦ ã‚¿ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(results)} ã‚¿ã‚¤ãƒ« / Loaded {len(results)} tiles")

        # è§£åƒåº¦ã‚’å–å¾— / Get resolution from sample tile
        sample = results[0]
        tile_rows, tile_cols = sample["elevations"].shape
        lat_res = (sample["lat_range"][1] - sample["lat_range"][0]) / tile_rows
        lon_res = (sample["lon_range"][1] - sample["lon_range"][0]) / tile_cols

        # å…¨ä½“ã®ç·¯åº¦çµŒåº¦ç¯„å›²ã‚’ç®—å‡º / Calculate overall lat/lon bounds
        lat_min_all = min(r["lat_range"][0] for r in results)
        lat_max_all = max(r["lat_range"][1] for r in results)
        lon_min_all = min(r["lon_range"][0] for r in results)
        lon_max_all = max(r["lon_range"][1] for r in results)

        # å…¨ä½“ã®é…åˆ—ã‚µã‚¤ã‚ºã‚’æ±ºå®š / Compute merged array size
        total_rows = int(round((lat_max_all - lat_min_all) / lat_res))
        total_cols = int(round((lon_max_all - lon_min_all) / lon_res))
        merged_array = np.full((total_rows, total_cols), np.nan)  # åˆæœŸå€¤ã¯ NaN / Initialize with NaNs

        # å„ã‚¿ã‚¤ãƒ«ã‚’æ­£ã—ã„ä½ç½®ã«æŒ¿å…¥ / Place each tile in correct position
        for r in results:
            elev = r["elevations"]
            lat_start, lat_end = r["lat_range"]
            lon_start, lon_end = r["lon_range"]

            row_start = int(round((lat_max_all - lat_end) / lat_res))
            row_end = row_start + elev.shape[0]
            col_start = int(round((lon_start - lon_min_all) / lon_res))
            col_end = col_start + elev.shape[1]

            merged_array[row_start:row_end, col_start:col_end] = elev

        print(f"ğŸ—º çµ±åˆå®Œäº†ï¼ shape: {merged_array.shape} / Merge complete!")

        # NaNã‚’ä¸€æ‹¬è£œé–“ï¼ˆæœ€å°å€¤ã§åŸ‹ã‚ã¦ã‹ã‚‰å¹³æ»‘åŒ–ï¼‰
        # Interpolate NaNs after merging (fill with min value, then smooth)
        if np.any(np.isnan(merged_array)):
            print("ğŸ”§ NaN ã‚’ä¸€æ‹¬è£œé–“ä¸­... / Interpolating NaN values...")
            filled = np.nan_to_num(merged_array, nan=np.nanmin(merged_array))
            smoothed = scipy.ndimage.gaussian_filter(filled, sigma=1)
            merged_array[np.isnan(merged_array)] = smoothed[np.isnan(merged_array)]
            print("âœ… è£œé–“å®Œäº† / Interpolation complete")

        # çµæœã‚’ä¿å­˜ / Save results
        np.save(f"{output_prefix}.npy", merged_array)
        np.savetxt(f"{output_prefix}.csv", merged_array, delimiter=",", fmt="%.2f")
        print(f"âœ… çµ±åˆé…åˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_prefix}.npy / .csv")

        # ç·¯åº¦çµŒåº¦ç¯„å›²ã‚’CSVã«ä¿å­˜ / Save lat/lon range to CSV
        with open("latlon_range.csv", mode="w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["lat_min", "lat_max", "lon_min", "lon_max"])
            writer.writerow([lat_min_all, lat_max_all, lon_min_all, lon_max_all])
        print("âœ… ç·¯åº¦çµŒåº¦ç¯„å›²ã‚’ latlon_range.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚ / Saved coordinate range to latlon_range.csv")

        return merged_array, lat_min_all, lat_max_all, lon_min_all, lon_max_all

     
if __name__ == "__main__":
    start = time.time()  # é–‹å§‹æ™‚åˆ»
    
    # ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    geo = CVRP_Geography("r2ka22223.topojson")
    print('ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–å®Œäº†', time.time() - start)
    
    
    # é«˜åº¦æƒ…å ±ã‚’csvåŠã³npyåŒ–ï¼ˆ1å›ã ã‘å®Ÿè¡Œã™ã‚Œã°OKï¼‰
    input_dir = "PackDLMap_xml_10m"
    elevation_array, lat_min, lat_max, lon_min, lon_max = geo.merge_gml_elevation_tiles_10m(
        input_dir,
        output_prefix="PackDLMap_xml_10m_output"
    )
    print('é«˜åº¦æƒ…å ±ã‚’csvåŠã³npyåŒ–ï¼ˆ1å›ã ã‘å®Ÿè¡Œï¼‰', time.time() - start)
    
    
    # é«˜åº¦ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    geo.load_elev("PackDLMap_xml_10m_output.npy", "latlon_range.csv")
    print('é«˜åº¦èª­ã¿è¾¼ã¿çµ‚äº†', time.time() - start)
    
    elev = 0
    #å¯¾è±¡é«˜åº¦(m)ä»¥ä¸‹ã‚’å‰Šé™¤ã™ã‚‹
    geo.get_filtered_road_network(output_file=f"omaezaki_â‰¤{elev}melev.graphml", elev=elev, nrate=0.5)#exclude_types=["trunk"], 
    print('ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†ä½œæˆ', time.time() - start)
    
    # å¯è¦–åŒ–
    geo.plot_colored_roads(f"omaezaki_â‰¤{elev}melev.graphml", f"omaezaki_road_map_â‰¤{elev}melev.png")
    print('å¯è¦–åŒ–', time.time() - start)
    
    # å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    graphml_file = f"omaezaki_â‰¤{elev}melev.graphml"  # GraphMLãƒ•ã‚¡ã‚¤ãƒ«
    nodes_csv = "omaezaki_nodes.csv"  # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿CSV
    output_csv = "omaezaki_travel_time.csv"  # é€šå¸¸å½¢å¼ã®ä¿å­˜å…ˆ
    output_matrix_csv = "omaezaki_travel_time_matrix.csv"  # è¡Œåˆ—å½¢å¼ã®ä¿å­˜å…ˆ
    # ç§»å‹•æ™‚é–“ã®è¨ˆç®—ã¨ä¿å­˜(æ™‚é–“ãŒã‹ã‹ã‚‹)
    geo.calculate_travel_times(graphml_file, nodes_csv, output_csv, output_matrix_csv)

    try:
        travel_time_matrix = pd.read_csv(output_matrix_csv, index_col=0)
        # è¡Œåˆ—ã®å€¤ã ã‘ã‚’å–å¾—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚„åˆ—åã‚’é™¤ãï¼‰
        matrix_values = travel_time_matrix.values
        # ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
        print(matrix_values)
        # ä¸Šä¸‰è§’è¡Œåˆ—ã‚’å¯¾ç§°è¡Œåˆ—ã«å¤‰æ›ã™ã‚‹é–¢æ•°
        def make_symmetric(matrix):
            # ä¸Šä¸‰è§’éƒ¨åˆ†ã‚’ä¸‹ä¸‰è§’ã«ã‚³ãƒ”ãƒ¼ã—ã¦å¯¾ç§°è¡Œåˆ—ã‚’ä½œæˆ
            symmetric_matrix = matrix + matrix.T - np.diag(np.diag(matrix))
            return symmetric_matrix

        # å¯¾ç§°è¡Œåˆ—ã‚’ä½œæˆ
        symmetric_matrix = make_symmetric(np.array(matrix_values))

        # çµæœã‚’è¡¨ç¤º
        print("Symmetric Matrix:")
        print(symmetric_matrix)

        # å¯¾ç§°è¡Œåˆ—ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        symmetric_matrix_df = pd.DataFrame(symmetric_matrix, index=travel_time_matrix.index, columns=travel_time_matrix.columns)
        symmetric_matrix_df.to_csv("omaezaki_symmetric_travel_time_matrix.csv")

    except FileNotFoundError:
        geo.calculate_travel_times(graphml_file, nodes_csv, output_csv, output_matrix_csv)

    
    print('å‡¦ç†å®Œäº†', time.time() - start)
    
    
